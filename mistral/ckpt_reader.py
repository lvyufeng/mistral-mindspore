import os
import io
import sys
import pickle
import zipfile
import pathlib
import warnings
import mmap as mmap_lib
from itertools import accumulate

from enum import Enum
from typing import Dict, Union, Optional, Sequence
from functools import reduce
import operator
import shutil
# from bfloat16 import bfloat16
from ml_dtypes import bfloat16

import numpy as np
import mindspore

def bfloat16_to_float32(
    data: Union[np.int16, np.int32, np.ndarray],
    dims: Optional[Union[int, Sequence[int]]] = None,
) -> np.ndarray:
    """Converts ndarray of bf16 (as uint32) to f32 (as uint32).

    :param data: a numpy array, empty dimensions are allowed if dims is None
    :param dims: if specified, the function reshapes the results
    :return: a numpy array of float32 with the same dimension if dims is None,
        or reshaped to dims if specified"""
    shift = lambda x: x << 16  # noqa: E731
    if dims is None:
        if len(data.shape) == 0:
            return shift(np.array([data]).astype(np.int32)).view(np.float32)[0]  # type: ignore[no-any-return]
        return shift(data.astype(np.int32)).view(np.float32)  # type: ignore[no-any-return]
    return shift(data.astype(np.int32)).reshape(dims).view(np.float32)  # type: ignore[no-any-return]

class PyTorchFileReader:
    """
    Class to allow PackageImporter to operate on unzipped packages. Methods
    copy the behavior of the internal PyTorchFileReader class (which is used for
    accessing packages in all other cases).

    N.B.: ScriptObjects are not depickleable or accessible via this DirectoryReader
    class due to ScriptObjects requiring an actual PyTorchFileReader instance.
    """

    def __init__(self, file):
        self.file = zipfile.ZipFile(file)
        self.directory = self.file.namelist()[0].split('/')[0]

    def open_record(self, name, extract=False):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            if extract:
                path = '/tmp/' + filename
                if os.path.exists(path):
                    return path
                return self.file.extract(filename, '/tmp/')
            return self.file.open(filename)
        return None

    def read_record(self, name):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            return self.file.read(filename)
        return None


    def has_record(self, name):
        filename = f"{self.directory}/{name}"
        return filename in self.file.namelist()

    def get_all_records(
        self,
    ):
        files = [name.replace(self.directory + '/' , '')for name in self.file.namelist()]
        return files

    def get_record_offset(self, name):
        filename = f"{self.directory}/{name}"
        if filename in self.file.namelist():
            return self.file.getinfo(filename).header_offset
        return None

class LoadEndianness(Enum):
    NATIVE = 1
    LITTLE = 2
    BIG = 3

_default_load_endian: Optional[LoadEndianness] = None

def get_default_load_endianness() -> Optional[LoadEndianness]:
    '''
    Get fallback byte order for loading files

    If byteorder mark is not present in saved checkpoint,
    this byte order is used as fallback.
    By default, it's "native" byte order.

    Returns:
        default_load_endian: Optional[LoadEndianness]
    '''
    return _default_load_endian

def set_default_load_endianness(endianness):
    '''
    Set fallback byte order for loading files

    If byteorder mark is not present in saved checkpoint,
    this byte order is used as fallback.
    By default, it's "native" byte order.

    Args:
        endianness: the new fallback byte order
    '''
    global _default_load_endian
    if not isinstance(endianness, LoadEndianness) and endianness is not None:
        raise TypeError("Invalid argument type in function set_default_load_endianness")
    _default_load_endian = endianness

def _is_zipfile(f) -> bool:
    # This is a stricter implementation than zipfile.is_zipfile().
    # zipfile.is_zipfile() is True if the magic number appears anywhere in the
    # binary. Since we expect the files here to be generated by torch.save or
    # torch.jit.save, it's safe to only check the start bytes and avoid
    # collisions and assume the zip has only 1 file.
    # See bugs.python.org/issue28494.

    # Read the first 4 bytes of the file
    read_bytes = []
    start = f.tell()

    byte = f.read(1)
    while byte != b"":
        read_bytes.append(byte)
        if len(read_bytes) == 4:
            break
        byte = f.read(1)
    f.seek(start)

    local_header_magic_number = [b'P', b'K', b'\x03', b'\x04']
    return read_bytes == local_header_magic_number

def _check_seekable(f) -> bool:

    def raise_err_msg(patterns, e):
        for p in patterns:
            if p in str(e):
                msg = (str(e) + ". You can only torch.load from a file that is seekable."
                                + " Please pre-load the data into a buffer like io.BytesIO and"
                                + " try to load from it instead.")
                raise type(e)(msg)
        raise e

    try:
        f.seek(f.tell())
        return True
    except (io.UnsupportedOperation, AttributeError) as e:
        raise_err_msg(["seek", "tell"], e)
    return False

def _is_path(name_or_buffer):
    return isinstance(name_or_buffer, (str, pathlib.Path))

def _is_torchscript_zip(zip_file):
    return 'constants.pkl' in zip_file.get_all_records()

class _opener:
    def __init__(self, file_like):
        self.file_like = file_like

    def __enter__(self):
        return self.file_like

    def __exit__(self, *args):
        pass


class _open_file(_opener):
    def __init__(self, name, mode):
        super().__init__(open(name, mode))

    def __exit__(self, *args):
        self.file_like.close()


class _open_buffer_reader(_opener):
    def __init__(self, buffer):
        super().__init__(buffer)
        _check_seekable(buffer)


class _open_buffer_writer(_opener):
    def __exit__(self, *args):
        self.file_like.flush()


def _open_file_like(name_or_buffer, mode):
    if _is_path(name_or_buffer):
        return _open_file(name_or_buffer, mode)
    else:
        if 'w' in mode:
            return _open_buffer_writer(name_or_buffer)
        elif 'r' in mode:
            return _open_buffer_reader(name_or_buffer)
        else:
            raise RuntimeError(f"Expected 'r' or 'w' in mode but got {mode}")

class _open_zipfile_reader(_opener):
    def __init__(self, name_or_buffer) -> None:
        super().__init__(PyTorchFileReader(name_or_buffer))

load_module_mapping: Dict[str, str] = {
    # See https://github.com/pytorch/pytorch/pull/51633
    'torch.tensor': 'torch._tensor'
}

def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata=None):
    num_elemets = reduce(operator.mul, size)
    array = storage[storage_offset: storage_offset + num_elemets]
    array = array.reshape(size)
    # array = array.astype(np.float16)
    # print(array.dtype, array)
    # exit()
    return array
    # tensor = mindspore.Tensor(array)
    # return mindspore.Parameter(tensor, requires_grad=requires_grad)

def _maybe_decode_ascii(bytes_str: Union[bytes, str]) -> str:
    # When using encoding='bytes' in Py3, some **internal** keys stored as
    # strings in Py2 are loaded as bytes. This function decodes them with
    # ascii encoding, one that Py3 uses by default.
    #
    # NOTE: This should only be used on internal keys (e.g., `typename` and
    #       `location` in `persistent_load` below!
    if isinstance(bytes_str, bytes):
        return bytes_str.decode('ascii')
    return bytes_str


dtype_map = {
    "HalfStorage": np.float16,
    "FloatStorage": np.float32,
    # 'BFloat16Storage': np.int16
    'BFloat16Storage': bfloat16
}

element_size_map = {
    "HalfStorage": 2,
    "FloatStorage": 3,
    'BFloat16Storage': 2
}

def load(f, map_location=None, pickle_module=pickle, *, weights_only=False, mmap=None, **pickle_load_args):
    if weights_only:
        if pickle_module is not None:
            raise RuntimeError("Can not safely load weights when explicit pickle_module is specified")
    else:
        if pickle_module is None:
            pickle_module = pickle

    # make flipping default BC-compatible
    if mmap is None:
        mmap = False

    if 'encoding' not in pickle_load_args.keys():
        pickle_load_args['encoding'] = 'utf-8'

    with _open_file_like(f, 'rb') as opened_file:
        if _is_zipfile(opened_file):
            # The zipfile reader is going to advance the current file position.
            # If we want to actually tail call to torch.jit.load, we need to
            # reset back to the original position.
            overall_storage = None
            with _open_zipfile_reader(opened_file, ) as opened_zipfile:
                if _is_torchscript_zip(opened_zipfile):
                    raise ValueError('do not support torchscript now')
                if mmap:
                    if not isinstance(f, str):
                        raise ValueError("f must be a string filename in order to use mmap argument")
                    size = os.path.getsize(f)
                    overall_storage = f
                if weights_only:
                    # try:
                    #     return _load(opened_zipfile,
                    #                  map_location,
                    #                  _weights_only_unpickler,
                    #                  overall_storage=overall_storage,
                    #                  **pickle_load_args)
                    # except RuntimeError as e:
                    #     raise pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from None
                    pass
                return _load(opened_zipfile,
                             pickle_module,
                             overall_storage=overall_storage,
                             **pickle_load_args)
        if mmap:
            raise RuntimeError("mmap can only be used with files saved with ",
                               "`torch.save(_use_new_zipfile_serialization=True), "
                               "please torch.save your checkpoint with this option in order to use mmap.")
        # if weights_only:
        #     try:
        #         return _legacy_load(opened_file, map_location, _weights_only_unpickler, **pickle_load_args)
        #     except RuntimeError as e:
        #         raise pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from None
        # return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)


def _load(zip_file, pickle_module, overall_storage=None, pickle_file='data.pkl', **pickle_load_args):
    loaded_storages = {}
    # check if byteswapping is needed
    byteordername = 'byteorder'
    byteorderdata = None
    if zip_file.has_record(byteordername):
        byteorderdata = zip_file.read_record(byteordername)
        if byteorderdata not in [b'little', b'big']:
            raise ValueError('Unknown endianness type: ' + byteorderdata.decode())
    elif get_default_load_endianness() == LoadEndianness.LITTLE or \
            get_default_load_endianness() is None:
        byteorderdata = b'little'
    elif get_default_load_endianness() == LoadEndianness.BIG:
        byteorderdata = b'big'
    elif get_default_load_endianness() == LoadEndianness.NATIVE:
        pass
    else:
        raise ValueError('Invalid load endianness type')

    if not zip_file.has_record(byteordername) and \
            get_default_load_endianness() is None and \
            sys.byteorder == 'big':
        # Default behaviour was changed
        # See https://github.com/pytorch/pytorch/issues/101688
        warnings.warn("The default load endianness for checkpoints without a byteorder mark "
                      "on big endian machines was changed from 'native' to 'little' endian, "
                      "to avoid this behavior please use "
                      "torch.serialization.set_default_load_endianness to set "
                      "the desired default load endianness",
                      UserWarning)

    def persistent_load(saved_id):
        assert isinstance(saved_id, tuple)
        typename = _maybe_decode_ascii(saved_id[0])
        data = saved_id[1:]

        assert typename == 'storage', \
            f"Unknown typename for persistent_load, expected 'storage' but got '{typename}'"
        storage_type, key, location, numel = data
        # print(storage_type, key, location, numel)
        # name = f'/data1/lvyufeng/llama-7b/pytorch_model-00001-of-00002/data/{key}'
        name = f'data/{key}'
        if name in loaded_storages:
            return loaded_storages[name]
        # if mmap:
        #     path = zip_file.open_record(name, extract=True)
        #     array = np.memmap(path, dtype_map[storage_type])
        # else:
        if overall_storage is not None:
            # print(numel * element_size_map[storage_type])
            array = np.memmap(overall_storage, dtype=dtype_map[storage_type], offset=zip_file.open_record(name)._fileobj.tell(), shape=(numel,))
            # array = np.frombuffer(overall_storage.read(numel * element_size_map[storage_type]), dtype_map[storage_type])
        else:
            array = np.frombuffer(zip_file.read_record(name), dtype_map[storage_type])

        # if storage_type == 'BFloat16Storage':
            # array = bfloat16_to_float32(array)
            # array = array.astype(np.float16)

        loaded_storages[name] = array
        return array

    load_module_mapping: Dict[str, str] = {
        # See https://github.com/pytorch/pytorch/pull/51633
        'torch.tensor': 'torch._tensor'
    }

    # Need to subclass Unpickler instead of directly monkey-patching the find_class method
    # because it's marked readonly in pickle.
    # The type: ignore is because mypy can't statically determine the type of this class.
    class UnpicklerWrapper(pickle_module.Unpickler):  # type: ignore[name-defined]
        # from https://stackoverflow.com/questions/13398462/unpickling-python-objects-with-a-changed-module-path/13405732
        # Lets us override the imports that pickle uses when unpickling an object.
        # This is useful for maintaining BC if we change a module path that tensor instantiation relies on.
        def find_class(self, mod_name, name):
            if mod_name == 'torch._utils':
                return eval(name)
            if mod_name == 'torch':
                return str(name)

            mod_name = load_module_mapping.get(mod_name, mod_name)
            return super().find_class(mod_name, name)

    # Load the data (which may in turn use `persistent_load` to load tensors)
    data_file = zip_file.open_record(pickle_file)

    unpickler = UnpicklerWrapper(data_file, **pickle_load_args)
    unpickler.persistent_load = persistent_load
    result = unpickler.load()

    return result
